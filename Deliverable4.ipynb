{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014ae5df",
   "metadata": {},
   "source": [
    "# AI‑Powered Job Market Insights — Consolidated Analysis\n",
    "\n",
    "Setup,\n",
    "Data Load,\n",
    "EDA,\n",
    "Modeling (regression, classification, clustering, association rules), Results & Conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68aad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, json, itertools, textwrap, warnings, pathlib, statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Association rules\n",
    "try:\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "except Exception as e:\n",
    "    print(\"mlxtend not available. If association rules are needed, install mlxtend.\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"/mnt/data/dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a07609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(\"Rows, Columns:\", df.shape)\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96feb737",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (from Deliverable 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9a5e2",
   "metadata": {},
   "source": [
    "# AI-Powered Job Market Insights: Exploring Automation Risk, Skill Demands, and Future Employment Trends\n",
    "## Introduction\n",
    "   The dataset I chose was the \"AI-Powered Job Market Insights\" from https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights. This dataset provides a synthetic yet highly realistic snapshot of today's evolving job market, with a special emphasis on artificial intelligence (AI) and automation across various industries. This dataset includes 500 records and 10 attributes, such as job title, industry, company size, location, AI adoption level, automation risk, required skills, salary, remote friendliness, and job growth projection. \n",
    "   \n",
    "   I chose this dataset because Ai's impact on the future of work is a widely discussed topic, and I wanted to explore which jobs are most at risk of automation, how AI adoption caries across industries, and what skills are becoming more critical as AI continues to evolve. \n",
    "   \n",
    "   This dataset is well-suited for this project because it meets the requirements of having at least 8-10 attributes and 500+ records, allowing for a comprehensive application of data mining techniques, including data preprocessing, feature engineering, regression modeling, classification, clustering, and association rule mining. This dataset's attributes makes it ideal for uncovering meaningful trends and deriving insights that align with the project's objective of solving real-world problem through data analysis.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd39f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only gives you 5 entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives you a summary of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c54fe",
   "metadata": {},
   "source": [
    "We are just gonna see how many of each job titles there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae316d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows how many of each jobs there are\n",
    "df.Job_Title.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce92714",
   "metadata": {},
   "source": [
    "Find the total number of mising values in each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6594619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cefa1d0",
   "metadata": {},
   "source": [
    "After checking for missing values, we found that the column are complete with no missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb4b45",
   "metadata": {},
   "source": [
    "We are going to now remove any duplicates by using the drop_duplicates() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992b5b7",
   "metadata": {},
   "source": [
    "Check for any inconsistent data by using the .unique() method to make sure that values are unique for each column making sure that only what is expected will be printed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Job_Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742132a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Company_Size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AI_Adoption_Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Automation_Risk'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf198cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Remote_Friendly'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Job_Growth_Projection'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc48c77",
   "metadata": {},
   "source": [
    "Check to see if there are any inconsistant values in the different columns, for examples if some entries were tech instead of technology. But since the columns are consistant we are good to move on to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a71751",
   "metadata": {},
   "source": [
    "round to the nearest whole number since more salaries are reported in whole dollars, this makes it easier to read and interpret. Showing the cents doesn't really add meaningful precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary_USD']=df['Salary_USD'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9419f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary_USD'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cf051",
   "metadata": {},
   "source": [
    "To check for potential noisy data, we can check for possible outliers. We can use the IQR (interquartile range) rule.\n",
    "\n",
    "First we will get the IQR with this equation: \n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "= 103,971.75 - 78,511.25\n",
    "\n",
    "= 25,460.50\n",
    "\n",
    "Then we find the Upper bound: \n",
    "\n",
    "Q3 + 1.5 * IQR\n",
    "\n",
    "= 103,971.75 + (1.5 * 25,460.50)\n",
    "\n",
    "= 103,971.75 + 38,190.75\n",
    "\n",
    "= 142,162.50\n",
    "\n",
    "Then we find the lower bound:\n",
    "\n",
    "Q1 - 1.5 * IQR\n",
    "\n",
    "= 78,511.25 - (1.5 * 25,460.50)\n",
    "\n",
    "= 78,511.25 - 38,190.75\n",
    "\n",
    "= 40,320.50\n",
    "\n",
    "The max value which is 155,210 is above the upper bound which could be a possible high-end outlier\n",
    "\n",
    "The min value which is 31,970 is below the lower bound which could be a possible low-end outlier\n",
    "\n",
    "These don't necessary mean that these are noisy data, the high-end outlier could be for a executive role and the low-end outlier could be an entry level role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays a count plot of which industry had the most jobs\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=df['Industry'], order=df['Industry'].value_counts().index)\n",
    "plt.title('Job Counts by Industry')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cd84f",
   "metadata": {},
   "source": [
    "In the graph above it shows that there are alot more job in the Manufacturing industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b22975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays a count plot of the number of each job title\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=df['Job_Title'], order=df['Job_Title'].value_counts().index)\n",
    "plt.title('Counts of Job Title')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Job Title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51889ce0",
   "metadata": {},
   "source": [
    "In the graph above it shows that there are alot more jobs as data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of occurrences of each unique value \n",
    "adoption_counts = df['AI_Adoption_Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "#creates a pie chart\n",
    "plt.pie(adoption_counts, labels=adoption_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of AI Adoption Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d97e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a figure for the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "#using seaborns countplot which industry is using AI the most\n",
    "sns.countplot(data=df, x='AI_Adoption_Level', hue='Industry', order=['Low', 'Medium', 'High'])\n",
    "plt.title('Distribution of AI Adoption by Industry')\n",
    "plt.xlabel('AI Adoption Level')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Industry', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a269db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "#creates a goruped count plot using seaborn \n",
    "sns.countplot(data=df, x='AI_Adoption_Level', hue='Job_Title', order=['Low', 'Medium', 'High'])\n",
    "plt.title('Distribution of AI Adoption by Job Title')\n",
    "plt.xlabel('AI Adoption Level')\n",
    "plt.ylabel('Count')\n",
    "#adds the key outside of the figure\n",
    "plt.legend(title='Job Titles',bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new dataframe containing only rows where 'Job_Growth_Projection' is 'Growth'\n",
    "growth_df = df[df['Job_Growth_Projection'] == 'Growth']\n",
    "plt.figure(figsize=(12, 6))\n",
    "#creates a count plot showing the number of 'Growth' jobs in each industry\n",
    "sns.countplot(x='Industry', data=growth_df, order=growth_df['Industry'].value_counts().index)\n",
    "#adds title\n",
    "plt.title('Industry Distribution in Jobs with \"Growth\" Projection')\n",
    "#add the title for x axis\n",
    "plt.xlabel('Industry')\n",
    "#add the title for y axis\n",
    "plt.ylabel('Count')\n",
    "#rotates the labels for the x axis so everything fits\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93826c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_df = df[df['Job_Growth_Projection'] == 'Decline']\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Industry', data=growth_df, order=growth_df['Industry'].value_counts().index)\n",
    "plt.title('Industry Distribution in Jobs with \"Decline\" Projection')\n",
    "plt.xlabel('Industry')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_df = df[df['Job_Growth_Projection'] == 'Growth']\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Job_Title', data=growth_df, order=growth_df['Job_Title'].value_counts().index)\n",
    "plt.title('Job Titles with \"Growth\" Projection')\n",
    "plt.xlabel('Job Titles')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the summary stats for the salary column\n",
    "print(df['Salary_USD'].describe())\n",
    "\n",
    "#gives you the first and thrid quartile\n",
    "Q1 = df['Salary_USD'].quantile(0.25)\n",
    "Q3 = df['Salary_USD'].quantile(0.75)\n",
    "\n",
    "#gives the interquartile range\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#gives you the lower and upper bound\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#this identifies the outliers for the salaries \n",
    "outliers = df[(df['Salary_USD'] < lower_bound) | (df['Salary_USD'] > upper_bound)]\n",
    "print(outliers[['Job_Title', 'Industry', 'Salary_USD']])\n",
    "\n",
    "#plots a boxplot to show salary distribution and outliers\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=df['Salary_USD'])\n",
    "plt.title('Salary Boxplot (Detecting Outliers)')\n",
    "plt.xlabel('Salary (USD)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10825b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_salary_row = df[df['Salary_USD'] == df['Salary_USD'].max()]\n",
    "print(max_salary_row[['Job_Title', 'Industry', 'Location', 'Company_Size', 'Salary_USD']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d18d25",
   "metadata": {},
   "source": [
    "There are a couple high-end and low-end outliers that I will need to look into, but for the time being I will assume that because one of the highest paying jobs is in Finance it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab833d5b",
   "metadata": {},
   "source": [
    " From my EDA process I found that most salaries range between $78k and $140K with a few high outliers like Marketing Specialist in Finace at $155k and low outliers like Data Scientist at $32K. Outliers may represent special postions and need careful treatment to avoid skewing models. \n",
    " \n",
    " From the job title distribution data scientist is the most common job title, followed by HR Manager, Cybersecurity Analyst, and others. This suggest focus areas for building predicitve models like predicitng salaries or growth for these popular roles.  Job title could be a strong predictive feature for job growth potention.\n",
    " \n",
    " From the AI adoption distribution, different job titles and industries show varying adoption levels, for example high AI adoption is more common in Technology and Finance and is lower in entertainment and retail. Interaction between AI adoption and industry may affect job risk or growth potential. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb008c7",
   "metadata": {},
   "source": [
    "Since machine learning models operate only on numerical data, feature engineering is essential for converting raw variables into a format that models can interpret effectively. This process plays a critical role in enhancing the model's performance, accuracy, and overall reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0eca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode Industry, Job_Title, AI_Adoption_Level, etc.\n",
    "df_encoded = pd.get_dummies(df, columns=['Industry', 'Job_Title', 'Company_Size', \n",
    "                                         'Location', 'AI_Adoption_Level', \n",
    "                                         'Remote_Friendly', 'Job_Growth_Projection'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b388522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the risk levels to nummerical values where Low = 1, Med = 2, and High = 3\n",
    "risk_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "df['Automation_Risk_Level'] = df['Automation_Risk'].map(risk_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Automation_Risk_Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7517a",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation (from Deliverable 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43327302",
   "metadata": {},
   "source": [
    "## Deliverable 2: Regression Modeling and Performance Evaluation\n",
    "Anna Bottu\n",
    "\n",
    "MSCS-634: Advanced Big Data and Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert remote-friendliness into a binary feature.\n",
    "df['Remote_Friendly_Binary'] = df['Remote_Friendly'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_mapping = {'Decline': -1, 'Stable': 0, 'Growth': 1}\n",
    "df['Job_Growth_Score'] = df['Job_Growth_Projection'].map(growth_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Skill_Count'] = df['Required_Skills'].apply(lambda x: len(x.split(',')) if pd.notnull(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adoption_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "df['AI_Adoption_Score'] = df['AI_Adoption_Level'].map(adoption_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8369e2",
   "metadata": {},
   "source": [
    "From our results we can see that the MAE is 0.71 which means that we are off by 0.71 from the actual job growth score. Since the job growth scores range from -1 to 1, this is a big error. The MSE value just measures larger error more heavily, the value we go was of 0.69, because this is closer to the max target range of 1, this means that there is poor predicition quality. The RMSE value was 0.83 which also tells us that there is a high averge error. Lastly the r^2 value of 0.008, because of this score being so close to 0 this tells us the model barely shows any useful patterns. All these values indicate that these metrics together show us that the AI adoption Score alone is a very poor predictor of job growth in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674effbd",
   "metadata": {},
   "source": [
    "These r^2 scores are mostly negative or very close to 0, which means that the model's performace is unstable and does not generalize well. Due to our mean being negative, that suggests that the model's predictions are less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = df[['AI_Adoption_Score']]   # predictor\n",
    "y = df['Job_Growth_Score']      # target\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "\n",
    "# Create jittered version of AI_Adoption_Score\n",
    "jittered_x = X_test['AI_Adoption_Score'] + np.random.normal(0, 0.1, size=len(X_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=jittered_x, y=y_test, color='black', label='Actual (jittered)')\n",
    "sns.lineplot(x=X_test['AI_Adoption_Score'], y=y_pred_lin, color='red', label='Predicted')\n",
    "plt.title('Linear Regression: AI Adoption vs Job Growth')\n",
    "plt.xlabel('AI Adoption Score')\n",
    "plt.ylabel('Job Growth Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim(0.5, 3.5)   # expand x-axis\n",
    "plt.ylim(-1.2, 1.2)  # expand y-axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cdecc",
   "metadata": {},
   "source": [
    "As you can see thse scores are similar to the linear regession model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports you need for these cells\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Features & target (adjust column names if yours differ)\n",
    "X = df[['AI_Adoption_Score']]\n",
    "y = df['Job_Growth_Score']\n",
    "\n",
    "# 2) Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Fit models\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),   # scale single feature before Ridge\n",
    "    ('reg', Ridge(alpha=1.0, random_state=42))\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "# 4) Predictions (these are what your plot uses)\n",
    "y_pred_lin   = lin_reg.predict(X_test)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# 5) Jitter for the scatter (re-use in both plots)\n",
    "jittered_x = X_test['AI_Adoption_Score'] + np.random.normal(0, 0.1, size=len(X_test))\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=jittered_x, y=y_test, color='black', label='Actual (jittered)')\n",
    "sns.lineplot(x=X_test['AI_Adoption_Score'], y=y_pred_ridge, color='red', label='Predicted (Ridge)')\n",
    "plt.title('Ridge Regression: AI Adoption vs Job Growth')\n",
    "plt.xlabel('AI Adoption Score')\n",
    "plt.ylabel('Job Growth Score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 3.5)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bccde",
   "metadata": {},
   "source": [
    "I explored whether AI adoption predicts job growth potential using two regression models, Linear Regression and Ridge Regression. Although my primary interest was in identifying correlation, building regression models helped confirm the relationship was weak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ed25d",
   "metadata": {},
   "source": [
    "Both regression models, Linear and Ridge, were used to evaluate the potential relationship between AI adoption and job growth. The r^2 scores for both models were near zero or negative, and cross-validation confirmed very poor generalization. This demonstrates that AI adoption level alone is not a strong predictor of job growth in this dataset. Therefore, while the models fulfill the requirement of regression analysis, they collectively reinforce our key insight: there is no significant correlation between AI adoption and job growth based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1980d64",
   "metadata": {},
   "source": [
    "Two regression models, the Linear Regression and Ridge Regression models were developed to examine whether AI adoption levels could predict job growth potential. The Linear Regression model yielded a mean absolute error (MAE) of 0.71, a mean squared error (MSE) of 0.69, a root mean squared error (RMSE) of 0.83, and a very low r^2 value of 0.008. Ridge Regression model yielded a MAE of 0.71, a MSE of 0.69, a RMSE of 0.83 and also a very low r^2 value of 0.008, which incorporates regularization to mitigate overfitting, produced similar results. Both models also demonstrated low or negative r^2 scores during cross-validation, indicating poor generalization to unseen data.\n",
    "\n",
    "These evaluation results suggest that AI Adoption Score alone does not serve as a meaningful predictor of job growth in this dataset. Overall, the analysis indicates that more complex modeling using additional features may be necessary to effectively predict job growth, as AI adoption on its own does not sufficiently explain the variation in growth outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54ee56",
   "metadata": {},
   "source": [
    "## Clustering & Association Rules (from Deliverable 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d333c0",
   "metadata": {},
   "source": [
    "The decision tress classifier performed poorly in predicting the job growth potential projections, achieving only 35% accuracy, which is close to random guessing across the three classes (low, medium, and high). The confusion matrix shows frequent misclassification, especially between classes 0 and 2, suggesting that the features used do not provide strong separation for this target and that more advanced models or better feature engineering may be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401ecb3",
   "metadata": {},
   "source": [
    "The k-NN classifier achieved an accuracy of 37%, which is only slightly better than random guessing across the three job growth projection classes. The confusion matrix shows that although the model correctly shows some instances particularly in Class 2, it frequently misclassified jobs across all categories, with overlap between Classes 0, 1, and 2. Overall, this indicates that k-NN struggles to separate the job growth projection groups, suggesting that the available features may not provide enough distinction for reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8613d4",
   "metadata": {},
   "source": [
    "The tuned decision tree classifier achieved a test accuracy of 35%, which is similar to the untuned version and only a little better than random guessing across the three job growth projection classes. Even after hyperparameter optimization, the confusion matrix shows frequent misclassifications, particularly between Classes 0 and 2, showing that the features used do not provide enough separation for accurate predictions. This indicates that decision trees, even when tuned, may not be the best model for this dataset, and more advanced methods or feature engineering may be needed to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f42258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model on test data.\n",
    "    Prints Accuracy, F1 score, and Classification Report.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred, average=\"weighted\")  # weighted for multi-class\n",
    "\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df406557",
   "metadata": {},
   "source": [
    "These results show that the tuned decision tree performed poorly, with an accuracy of about 35% and an F1 score of 0.35, which is only a little better than random guessing across the three job growth projection classes. The k-NN model performed a little better, reaching 41% accuracy and an F1 score of 0.40, but it still struggled to correctly show the difference between the classes, as seen in the confusion matrix where many predictions overlap. Overall, both models show that the current features do not give us a good prediction for job growth projection, and more advanced models or additional features may be needed to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick numeric features that actually exist\n",
    "X_features = df[['AI_Adoption_Level', 'Automation_Risk', 'Salary_USD', 'Job_Growth_Projection']]\n",
    "\n",
    "# If any of these have missing values, fill or drop (simple fill here)\n",
    "X_features = X_features.fillna(X_features.median(numeric_only=True))\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Elbow method\n",
    "inertia = []\n",
    "K_range = range(1, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K_range, inertia, marker=\"o\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia (Within-Cluster SSE)\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857af02a",
   "metadata": {},
   "source": [
    "The elbow method plot shows how the within-cluster variance decreases as the number of clusters increases. The curve starts to flatten around k = 3, which tells us that three clusters may be the best choice, as adding more clusters beyond that point will give us only small improvements. Grouping the data into three clusters gives a good balance between efficiency and accuracy, capturing the main structure of the dataset without overcomplicating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a291e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df[\"Cluster\"] = clusters\n",
    "\n",
    "print(df.groupby(\"Cluster\").mean())  # See feature patterns per cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23edcc",
   "metadata": {},
   "source": [
    "This K-Means model divided the dataset into three clusters, each with its own characteristics. Cluster 0 is made up of jobs with moderate salaries, slightly higher automation risk, and limited remote opportunities. Cluster 1 stands out with higher salaries, more specialized skill requirements, and the tendency to be remote-friendly, telling us that these are higher-skill roles with better flexibility. Cluster 2 also shows relatively strong salaries and advanced skill needs, but unlike Cluster 1, these jobs are less likely to be remote-friendly. The results tells us that while Clusters 1 and 2 have higher-paying, skill-intensive roles, Cluster 1 has the added advantage of remote work, making it attractive in today’s job market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682cb73",
   "metadata": {},
   "source": [
    "This visualization shows how the K-Means algorithm grouped the data into three distinct clusters after reducing the dimensions with PCA. Each color represents a cluster, and we can see that the points are well separated, which means the algorithm was able to detect patterns in the data. It confirms that the dataset naturally splits into three groups with different feature characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d423c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at average feature values per cluster\n",
    "cluster_summary = df.groupby(\"Cluster\").mean()\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=\"Cluster\", y=\"Salary_USD\", data=df)\n",
    "plt.title(\"Salary Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=\"Cluster\", hue=\"Automation_Risk\", data=df)\n",
    "plt.title(\"Automation Risk by Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1caf67",
   "metadata": {},
   "source": [
    "The clustering results shows three main groups of jobs that differ in salary levels and automation risk. Cluster 0 includes mid-level positions with salaries around 80,000–90,000, but with a lot of variation depending on the industry. These jobs fall across both stable roles and ones more vulnerable to automation. Cluster 1 stands out with higher salaries, often over 100,000, and lower automation risk, likely representing specialized, high-skill positions in fields like technology or management where pay is strong and job security is better. Cluster 2 is more mixed, with some well-paying roles but also many lower-salary jobs, and it carries a higher risk of automation. This cluster likely reflects roles in areas like sales, marketing, or operations, which may pay competitively now but could face challenges as automation grows. Overall, Cluster 1 appears to offer the best balance of pay and stability, while Cluster 2 highlights jobs where reskilling may be important in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip setuptools packaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# pick the categorial columns for the association rule mining\n",
    "cat_cols = [\n",
    "    \"Industry\",\"Company_Size\",\"Location\",\n",
    "    \"AI_Adoption_Level\",\"Automation_Risk\",\n",
    "    \"Required_Skills\",\"Remote_Friendly\",\"Job_Growth_Projection\"\n",
    "]\n",
    "#transforms the data into true/false values\n",
    "transactions = pd.get_dummies(df[cat_cols], drop_first=False).astype(bool)\n",
    "\n",
    "#makes sure that the dataset only has true and false\n",
    "assert set(transactions.stack().unique()) <= {True, False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309728cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequent itemsets\n",
    "frequent_itemsets = apriori(transactions, min_support=0.05, use_colnames=True)\n",
    "\n",
    "#association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "rules = rules.sort_values(\"lift\", ascending=False)\n",
    "\n",
    "rules[['antecedents','consequents','support','confidence','lift']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbdc12",
   "metadata": {},
   "source": [
    "The patterns uncovered through association rule mining show us several important trends that connect job characteristics with industry outcomes. One of the strongest findings is that remote-friendly jobs requiring specialized skills are more likely to be found in industries that are projected to grow. This suggests that for individuals, developing these in-demand skills can open up opportunities for stable, flexible careers, while for employers, promoting remote options can help attract talent in competitive fields. Another key insight is the strong link between AI adoption and industry expansion. Companies that invest in AI, especially those offering remote work and requiring advanced skills, are concentrated in industries with positive job growth projections. This has real implications where workers in high-automation-risk jobs can look toward reskilling opportunities in these areas, while companies can use this knowledge to plan retraining programs rather than downsizing. Larger companies with higher AI adoption also tend to demand more advanced technical and digital skills, which signals to job seekers the value of certifications in areas like cloud computing, cybersecurity, and AI. Taken together, these patterns emphasize that remote work, AI adoption, and advanced skill sets are strong indicators of where the most resilient and rewarding jobs are likely to be found, while also underscoring the need for adaptation and reskilling in roles more vulnerable to automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd94fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
